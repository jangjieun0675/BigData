# =============================================================================
# ※ 빅데이터 분석 기초 지식
# (1) 범주형 변수(질적 자료) : 범주형은 모두 숫자로 바꿔야 데이터 저리가 가능하다
#
# 명목형 변수 : 순서 없음, 값을 구분하기 위한 변수 (성별, 혈액형, 국가, 직업)
# 서열형(순위형) 변수 : 순서 있음, 순위형 (학점, 제품 만족도)
# (2) 수치형 변수(양적 자료)
# 
# 구간형(정수형) 변수 : 등간형(Interval), 셀 수는 있지만 특정 구간이 존재하는 변수, 사칙연산 (년도, 발생횟수, 자녀수)
# 비율형 변수 : 연속형(continuous), 연속적인 값을 가지며 차이와 비율에 의미가 있는 변수(소득, 키, 몸무게)
# ※ 독립변수와 종속변수
# - 독립변수 : 어떤 실험에서 실험자가 직접 변경하는 변수(결과에 영향을 주는 변수들)
# - 종속변수 : 독립변수의 값이 변함에 따라 달라지는 수량(결과값)
# 
# ex) 용돈을 벌기 위해 집안일을 돕는다고 가정합시다. 집안일 한 개당 용돈 300원을 받는다.
# 이때, 독립변수는 집안일의 양이고 종속변수는 집안일을 해서 버는 용돈(독립변수 * 300)이다.
# 
# ※ 상관관계와 인과관계
# 양의 상관관계	하나의 변수가 증가할 때 다른 변수가 함께 증가하는 경우
# 음의 상관관계	하나의 변수가 증가할 때 다른 변수가 함께 감소하는 경우
# 무 상관관계	두 변수 간의 아무런 증감이 없는 경우
#  
# 
# - 인과관계 분석(회귀 분석) : 선행하는 변수가 후행하는 다른 변수의 원인이 되고 있다고 믿어지는 관계
# 
# ▶ 인과관계가 있다면  → 상관관계도 존재, 그러나 상관관계가 있다고 해서 인과관계가 존재한다고는 할 수 없다.
# 
# 종속변수가 1개, 독립변수가 1개 → 단변량 단순 선형 회귀 모델
# 종속변수가 1개, 독립변수가 2개 → 단변량 다중 선형 회귀 모델
# 종속변수가 2개, 독립변수가 1개 → 다변량 단순 선형 회귀 모델
# 종속변수가 2개, 독립변수가 2개 → 다변량 다중 선형 회귀 모델
# ※ 선형 회귀 분석 : 주어진 데이터가 이루는 하나의 선을 찾는 것
# 
# 위와 같은 수식을 나타내며 x라는 독립변수가 Y라는 종속변수에 주는 영향력을 나타낸다.
# 
# 해당 수식에서 a는 x가 변해도 Y에 영향을 주지 않는 회귀 계수를 말하며 E는 오차항 b는 X에 영향력을 주는 계수이다.
# 
# ※ 분석 평가 지표
# 회귀 모델 성능 평가 기준
# 
# 0에 가까울수록 좋은 성능 : MAE, MSE, RMSE
# 1에 가까울수록 좋은 성능 : R-Squared
# 
# MAE : 실제값과 모델 예측값 차이를 절대값으로 변환한 후 평균낸 값
# 
# MSE : 실제값과 모델 예측값 차이를 제곱한 후 평균낸 값
# 
# RMSE : 예측 대상 크기의 영향을 많이 받아 MAE보다 특이치에 강하다
# 
# R2_score : 독립 변수가 종속 변수를 얼마나 잘 설명해주는지 보여주는 지표

# y값에서 예측한 y값의 평균은 항상 0이다. 그러므로 제곱하여 SSE를 구해야한다.
# =============================================================================
# =============================================================================
# ※ 회귀 분석과 분류 분석의 차이
# - 회귀 분석(regression) : y값이 수치형(숫자) 데이터
# 
# - 분류 분석(classification) : y값이 범주형(0과 1) 데이터
#
# (1) 이진 분류( logistic regression )
# 
# – 로지스틱 회귀 모델은 이진 분류 결과를 평가하기 위해 오차 행렬에 기반한 성능 지표인 정밀도, 재현율, F1 스코어, ROC_AUC를 사용
# 
# (2) 분류 분석( classification )
# 
# 1. 이진 분류( logistic regression )
# ※  시그모이드 함수
# – 로지스틱 회귀에서 사용하는 S자 함수
# 
# – x의 값이 커지면 y의 값은 1에 근사하게 되고 x의 값이 작아지면 y의 값은 0에 근사하게 되어 S자 형태의 그래프가 됨
# 
# – 두 개의 값을 분류하는 이진 분류에 많이 사용
# =============================================================================
#오차 행렬( 혼동행렬, Confusion Matrix )
# 각 지표 계산하는 식
# 정확도(Accuracy) : (TP + TN)/(TP + TN + FP + FN)
# 정밀도(Precision) : TP / (TP + FP)
# 재현율(Recall) : TP / (TP + FN)
# F1 Score : 2 * (Precision * Recall) / (Precision + Recall)
# FPR : FP / (FP + TN)
# 특이도(specificity) : 1 - FPR = TN / (TN + FP)

#EX) 100명 중 8명의 암환자가 있을 때 진단을 통해 10이 암이라고 판정했다.
# 10명중 6명은 실제 암이고 4명은 암환자가 아니었다.
#            예측(N)      예측(P)       # 긍정을 긍정으로 올바르게 예측 : TP
#실제(N=90)   TN = 88     FP = 4        # 부정을 부정으로 올바르게 예측 : TN
#실제(P=10)   FN = 2      TP = 6        # 긍정을 부정으로 잘못 예측 : FN
                                        # 부정을 긍정으로 잘못 예측 : FP

# 1에 가까울수록 정확히 예측하고 구분한다는 것
#정확도(Accuracy): 모델이 올바르게 분류한 샘플의 비율입니다. 이 값이 1에 가까울수록 모델이 더 많은 샘플을 올바르게 분류한 것입니다.
#정밀도(Precision): 양성으로 예측된 샘플 중 실제로 양성인 샘플의 비율입니다. 이 값이 1에 가까울수록 모델이 더 적은 수의 거짓 양성(False Positive)을 만들었다는 것을 의미합니다.
#재현율(Recall)= 민감도(sensitivity): 실제 양성 샘플 중 모델이 양성으로 예측한 샘플의 비율입니다. 이 값이 1에 가까울수록 모델이 더 적은 수의 거짓 음성(False Negative)을 만들었다는 것을 의미합니다.
#F1 점수(F1 Score): 1에 가까울수록 모델의 정밀도와 재현율이 모두 높다는 것을 의미합니다.
#ROC_AUC:1에 가까울수록 모델이 양성 클래스와 음성 클래스를 잘 구분한다는 것을 의미합니다.

# =============================================================================
# 2. 분류 분석( classification )
# 2-1. Decision Tree
# – 의사결정 트리는 기계학습에서 지도학습의 알고리즘으로 분류 또는 회귀 분석 목적을 사용
# 
# 목표변수 유형에 따른 의사결정 트리
# 
# - 범주형 목표변수 : 분류 트리(Classification Tree)
# 
#      예) 성별(남,여), 혈액형(A, B, O, AB) 등
# 
# - 연속형 목표변수 : 회귀 트리(Regression Tree)
# 
#       목표변수가 연속형인 경우, 평균과 표준편차에 기초해 분리 발생 → 회귀 트리 구성  예) 가격, 키, 비율 등
# ▶ Decision Tree 분리기준(Split Criterion)
# 
# - 부모 노드로부터 자식 노드들이 형성될 때, 생성된 자식 노드에 속하는 자료의 순수도(Purity)가 가장 크게 증가하도록 트리를 형성하며 진행
#
# ① 지니 지수(Gini Index)
# 
# - 데이터 집합의 불순도를 측정
# 
# - 0~1 사이의 값을 가지며, 어떤 데이터 집합에 속한 개체(레코드)들이 같은 범주(클래스로 구성되어 있으면 지니 지수는 최솟값이 0을 갖고 해당 데이터 집합은 순수하다고 볼 수 있음
# 
# - 지니 지수가 작을수록 잘 분류된 것으로 볼 수 있음!!!!!!!
# 
# ② 엔트로피 지수(Entropy Index)
# 
# - 엔트로피는 주어진 데이터 집합의 혼잡도를 의미
# 
# - 주어진 데이터 집합에 서로 다른 범주(클래스)의 개체(레코드)들이 많이 섞여 있으면 엔트로피가 높고, 같은 범주의 개체들이 많이 있으면 엔트로피가 낮음
# 
# - 엔트로피 지수는 0~1 사이의 값을 가지며, 가장 혼잡도가 높은 상태(서로 다른 범주의 개체들이 섞여 있는 상태)는 1, 혼잡도가 가장 낮은 상태(하나의 범주의 개체로 구성된 상태)는 0
# 
# - 엔트로피 지수가 작을수록 잘 분류된 것으로 볼 수 있음!!!!!!
# 
# ③ 정보 이득(Information Gain)
# 
# - 상위 노드의 엔트로피 지수에서 하위 노드의 가중평균한 엔트로피 지수를 뺀 것을 의미
# 
# - 즉, 원래 상위 노드의 엔트로피를 구하고 어떤 속성을 선택한 후의 x개의 하위 노드로 분리된 것에 대한 가중평균한 엔트로피를 구한 값의 차를 의미
#(계산되어 나온 값이 클 수록 정보 이득이 큰 것을 의미, 선택한 어떤 속성이 분류하기 좋다고 볼 수 있음!!!!!!!)
# =============================================================================
# =============================================================================
# 2-2. Random Forest
# ※ 앙상블( ensemble )
# 앙상블 학습 : 여러 개의 분류기를 생성하고 그 예측을 결합하여 더 좋은 분류기를 만드는 방법
# 
# Voting : 다른 알고리즘 분류기를 결합
# Bagging : 같은 분류기를 중복샘플링(Bootstrap Aggrating)여 결과를 결합(예 : 랜덤포레스트)
# 
# Random Forest 특징
# 
# 1) 여러 개의 Decision Tree를 결합함으로 단일 Decision Tree의 결점을 극복
# 2) Over-fitting 문제가 적음
# 3) 구현이 간단함
# 4) 병렬 계산이 간편함
# 
# • Random Forest는 여러 개의 decision tree를 결합해 하나의 모형을 생성
# 
# • Random Forest 모듈에서 tree의 개수가 많을수록 좋은 것은 아님
# 
# Random Forest에서의 2가지 random
# 
# 1) Dataset에서 샘플 데이터를 random으로 선택
# 2) 샘플 데이터에서 feature(독립변수=x값)를 random으로 선택해 decision tree를 생성
# 
# random으로 선택한 n개의 데이터 사이는 데이터 중복이 가능하다 
# =============================================================================
# =============================================================================
# 3. 군집 분석_cluster analysis
# ※ 비지도 학습
# – 훈련 데이터에 타깃값이 주어지지 않은 상태에서 학습을 수행하는 방식
# 
# – 훈련 데이터를 학습하여 모델을 생성하면서 유사한 특성(관계, 패턴 등)을 가지는 데이터를 클러스터로 구성
# 
# – 새로운 데이터의 특성을 분석하여 해당하는 클러스터를 예측
# 
# 
# ※ 군집화
# – 데이터를 클러스터(군집)으로 구성하는 작업
# 1. K-means (K-평균 알고리즘)
# - k개의 중심점을 임의 위치로 잡고 중심점을 기준으로 가까이 있는 데이터를 확인한 뒤 그들과의 거리의 평균 지점으로 중심점을 이동하는 방식

# ▶ 장점
# 
# 직관적이고 구현이 쉽다
# 대용량 데이터에 적용 가능하다
# ▶ 단점
# 
# 사전에 클러스터 개수와 초기값을 지정해야 한다
# 초기값에 따라 결과가 달라질 수 있다
# 데이터 양이 많아지면 수행시간이 오래걸린다
# 이상치에 영향을 받는다
# 범주형 변수가 있다면 K-Means 클러스터링을 할 수 없다(범주형 변수 → 수치형 변수로 변환해야함)
# 
# 2. 엘보 방법
# - 클러스터의 개수 k의 변환에 따른 왜곡의 변화를 그려보면 그래프가 꺽이는 지점인 엘보가 나타나는데, 그 지점의 k를 최적의 k로 선택
# 
# - 왜곡 : 클러스터의 중심점과 클러스터 내의 데이터 거리 차이의 제곱값의 합
# 
# 3. 실루엣 분석
# - 클러스터 내에 있는 데이터가 얼마나 조밀하게 모여있는지 측정하는 그래프 도구
# - a(i) : 데이터 i가 해당 클러스터 내의 데이터와 얼마나 가까운가를 나타내는 클러스터 응집력
# - b(i) : 가장 가까운 다른 클러스터 내의 데이터와 얼마나 떨어져있는가를 나타내는 클러스터 분리도
# - (-1)에서 1사이의 값을 가지며 1에 가까울수록 좋은 군집화를 의미
# 
# =============================================================================
# =============================================================================
# 4. 텍스트 마이닝_text mining
# 
# ※ 텍스트 마이닝
# – 비정형의 텍스트 데이터로부터 패턴을 찾아내어 의미 있는 정보를 추출하는 분석 과정 또는 기법
# 
# – 데이터 마이닝과 자연어 처리, 정보 검색 등의 분야가 결합된 분석 기법을 사용
# 
# – 텍스트 마이닝의 프로세스
# 
#    텍스트 전처리 → 특성 벡터화 → 머신러닝 모델 구축 및 학습/평가 프로세스 수행 
# 
#     » 텍스트 전처리에는 토큰화, 불용어 제거, 표제어 추출, 형태소 분석 등의 작업이 포함
# 
# ※ 특성 벡터화와 특성 추출
# –  머신러닝 알고리즘으로 분석하기 위해서는 텍스트를 구성하는 단어 기반의 특성 추출을 하고 이를 숫자형 값인 벡터 값 으로 표현해야 함
# 
# – 특성 벡터화의 대표적인 방법으로 BoW와 Word2ve가 있음
# 
# – BOW : 순서는 무시한 채 빈도만 고려하여 단어가 얼마나 자주 등장하는지로 특성 벡터를 만드는 방법
# 
#   카운트 기반 벡터화와 TF -IDF 기반 벡터화 방식이 있음
# (1) 카운터 기반 벡터화
# 
# - 출현빈도가 높을수록 중용한 단어로 다루어짐
# 
# - 문서 d에 등장한 단어 t의 횟수는 tf(t,d)로 표현
# 
# (2) TF-IDF 기반 벡터화
# 
# – 특정 문서에 많이 나타나는 단어는 해당 문서의 단어 벡터에 가중치를 높임
# 
# – 모든 문서에 많이 나타나는 단어는 범용적으로 사용하는 단어로 취급하여 가중치를 낮추는 방식
# 
# – (역문서 빈도) idf(t,d)
# 
# – df(d,t)는 단어 t가 포함된 문서 d의 개수
# 
# ★ TF-IDF 가중치는 다른 문서에 적게 나오고 자기 문서에 많이 나오면 높다.
# 
# 그러나 자기 문서에 많이나오나 다른 문서에도 많이 나오면 가중치는 낮다.
#      
# =============================================================================
     
     
     
     
     
     
     